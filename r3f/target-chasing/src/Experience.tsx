import { Html } from "@react-three/drei";
import { useFrame } from "@react-three/fiber";
import { RigidBody, RapierRigidBody } from "@react-three/rapier";
import { useRef, useState } from "react";
import { Vector3 } from "three";
import { DQNAgent } from "@ignitionai/backend-tfjs";
import { IgnitionEnv } from "@ignitionai/core";
import React from "react";

const MOVEMENT_SPEED = 100;

// Constantes pour les limites du plateau
const PLATEAU_LIMITS = {
  minX: -500, // Moiti√© de la largeur du ground (1000/2)
  maxX: 500,  // Moiti√© de la largeur du ground (1000/2)
  minY: -1,   // L√©g√®rement en-dessous du ground
  maxY: 100,  // Hauteur maximale
  minZ: -500, // Moiti√© de la profondeur du ground (1000/2)
  maxZ: 500   // Moiti√© de la profondeur du ground (1000/2)
};

function Ground() {
  return (
    <RigidBody type="fixed">
      <mesh receiveShadow position={[0, -0.5, 0]}>
        <boxGeometry args={[1000, 1, 1000]} />
        <meshStandardMaterial color="gray" />
      </mesh>
    </RigidBody>
  );
}

function Cible({cibleRef}: {cibleRef: React.RefObject<RapierRigidBody | null>}) {
  return (
    <RigidBody  
      ref={cibleRef}
      type="kinematicPosition" 
      position={[0, 10, 0]} 
      shape="cuboid"
      linearDamping={0.5}
      mass={1}
      friction={0}
      lockRotations
    >
      <mesh castShadow>
        <boxGeometry args={[20, 20, 20]} />
        <meshStandardMaterial color="red" />
      </mesh>
    </RigidBody>
  );
}

function Agent({targetPosition}: {targetPosition: [number, number, number]}) { 
  // R√©f√©rences et √©tats
  const bodyRef = useRef<RapierRigidBody>(null);
  const initialPosition: [number, number, number] = [100, 10, 100]; // Position de d√©part √©loign√©e
  
  // √âtats d'entra√Ænement
  const [isTraining, setIsTraining] = useState(false);
  const [episodeCount, setEpisodeCount] = useState(0);
  const [reward, setReward] = useState(0);
  const [bestReward, setBestReward] = useState(-Infinity);
  const [episodeSteps, setEpisodeSteps] = useState(0);
  const [reachedTarget, setReachedTarget] = useState(false);
  const [episodeTime, setEpisodeTime] = useState(0);
  const [episodeStartTime, setEpisodeStartTime] = useState(Date.now());
  const [successCount, setSuccessCount] = useState(0);
  const [difficulty, setDifficulty] = useState(0);
  const [lastAction, setLastAction] = useState(-1);
  
  // R√©f√©rence √† l'environnement (initialis√© une seule fois)
  const envRef = useRef<IgnitionEnv | null>(null);
  
  // R√©f√©rence √† la derni√®re distance pour calculer la progression
  const lastDistance = useRef(Infinity);
  
  // Calculer la position initiale en fonction de la difficult√©
  const getStartingPosition = (): [number, number, number] => {
    // Si difficult√© = 0, on commence tr√®s proche de la cible
    if (difficulty === 0) {
      return [targetPosition[0] + 30, targetPosition[1], targetPosition[2] + 30];
    }
    // Si difficult√© = 1, on commence √† distance moyenne
    else if (difficulty === 1) {
      return [targetPosition[0] + 60, targetPosition[1], targetPosition[2] + 60];
    }
    // Sinon, on commence √† la position normale
    return initialPosition;
  };
  
  // Cr√©er l'environnement s'il n'existe pas encore
  if (!envRef.current) {
    console.log("üîÑ Initialisation de l'environnement d'apprentissage");
    
    const dqnAgent = new DQNAgent({
      actionSize: 4,
      inputSize: 9,
      epsilon: 0.95,     // Tr√®s forte exploration au d√©but
      epsilonDecay: 0.99, // D√©croissance lente
      lr: 0.001,
      batchSize: 32,
      gamma: 0.99,
      hiddenLayers: [64, 64],
      memorySize: 100000,
      targetUpdateFrequency: 200,
    });
    
    envRef.current = new IgnitionEnv({
      agent: dqnAgent,
      getObservation: () => {
        const pos = bodyRef.current?.translation() || { x: 0, y: 0, z: 0 };
        
        // Direction vers la cible (vecteur normalis√©)
        const dirX = targetPosition[0] - pos.x;
        const dirY = targetPosition[1] - pos.y;
        const dirZ = targetPosition[2] - pos.z;
        const dirMagnitude = Math.sqrt(dirX*dirX + dirY*dirY + dirZ*dirZ);
        
        // Normaliser les distances aux limites
        const distToMinX = (pos.x - PLATEAU_LIMITS.minX) / 1000;
        const distToMaxX = (PLATEAU_LIMITS.maxX - pos.x) / 1000;
        const distToMinZ = (pos.z - PLATEAU_LIMITS.minZ) / 1000;
        const distToMaxZ = (PLATEAU_LIMITS.maxZ - pos.z) / 1000;
        const distToGround = (pos.y - PLATEAU_LIMITS.minY) / 100;
        
        return [
          dirX / (dirMagnitude + 0.001),  // Direction normalis√©e X vers la cible
          dirY / (dirMagnitude + 0.001),  // Direction normalis√©e Y vers la cible 
          dirZ / (dirMagnitude + 0.001),  // Direction normalis√©e Z vers la cible
          dirMagnitude / 1000,            // Distance normalis√©e
          distToMinX,                     // Distance √† la limite X minimale
          distToMaxX,                     // Distance √† la limite X maximale
          distToGround,                   // Distance au sol
          distToMinZ,                     // Distance √† la limite Z minimale
          distToMaxZ,                     // Distance √† la limite Z maximale
        ];
      },
      applyAction: (action: number | number[]) => {
        // Incr√©menter le compteur d'√©tapes √† chaque action
        setEpisodeSteps(prev => prev + 1);
        
        // Tester si le Rigidbody existe
        if (!bodyRef.current) {
          console.error("‚ùå bodyRef.current est null!");
          return;
        }
        
        // V√©rifier l'√©tat actuel de l'agent
        const pos = bodyRef.current.translation();
        
        // Ajouter un peu d'al√©atoire aux premi√®res √©tapes
        const randomFactor = Math.min(1, 100 / (episodeCount + 1));
        const shouldRandomize = Math.random() < randomFactor && episodeSteps < 5;
        
        if (Array.isArray(action)) {
          bodyRef.current.setLinvel(new Vector3(
            action[0] * MOVEMENT_SPEED, 
            action[1] * MOVEMENT_SPEED, 
            action[2] * MOVEMENT_SPEED
          ), true);
        } else {
          let finalAction = action;
          
          // Parfois, choisir une action al√©atoire au d√©but
          if (shouldRandomize) {
            finalAction = Math.floor(Math.random() * 4);
            console.log(`üé≤ Action randomis√©e: ${finalAction}`);
          }
          
          setLastAction(finalAction);
          
          // TR√àS grandes forces pour s'assurer que l'agent bouge
          const velocity = MOVEMENT_SPEED * 5;
          
          // Utiliser setLinvel pour un mouvement direct plut√¥t que des impulsions
          switch(finalAction) {
            case 0: // Gauche
              console.log("‚¨ÖÔ∏è Mouvement: GAUCHE");
              bodyRef.current.setLinvel(new Vector3(-velocity, 0, 0), true); 
              break;
            case 1: // Droite
              console.log("‚û°Ô∏è Mouvement: DROITE");
              bodyRef.current.setLinvel(new Vector3(velocity, 0, 0), true); 
              break; 
            case 2: // Avant (vers la cible sur Z)
              console.log("‚¨ÜÔ∏è Mouvement: AVANT");
              bodyRef.current.setLinvel(new Vector3(0, 0, -velocity), true); 
              break;
            case 3: // Arri√®re
              console.log("‚¨áÔ∏è Mouvement: ARRI√àRE");
              bodyRef.current.setLinvel(new Vector3(0, 0, velocity), true); 
              break;
            default:
              bodyRef.current.setLinvel(new Vector3(0, 0, 0), true);
          }
          
          // Forcer les logs √† chaque action pour d√©bogage
          console.log(`Action: ${finalAction}, Position: [${pos.x.toFixed(2)}, ${pos.y.toFixed(2)}, ${pos.z.toFixed(2)}]`);
        }
      },
      computeReward: () => {
        const pos = bodyRef.current?.translation() || { x: 0, y: 0, z: 0 };
        const distance = Math.sqrt(
          Math.pow(pos.x - targetPosition[0], 2) +
          Math.pow(pos.y - targetPosition[1], 2) +
          Math.pow(pos.z - targetPosition[2], 2)
        );
        
        // Calculer la r√©compense
        let calculatedReward = 0;
        
        // V√©rifier si l'agent est sorti du plateau - forte p√©nalit√©
        if (pos.x < PLATEAU_LIMITS.minX || pos.x > PLATEAU_LIMITS.maxX ||
            pos.y < PLATEAU_LIMITS.minY || pos.y > PLATEAU_LIMITS.maxY ||
            pos.z < PLATEAU_LIMITS.minZ || pos.z > PLATEAU_LIMITS.maxZ) {
          return -100; // P√©nalit√© pour √™tre sorti
        }
        
        // V√©rifier si l'agent a atteint la cible - √©norme r√©compense
        if (distance < 25) {
          console.log(`üèÜ TOUCH√â! Distance = ${distance.toFixed(2)}`);
          setReachedTarget(true);
          // Incr√©menter imm√©diatement le succ√®s pour s'assurer qu'il est compt√©
          setSuccessCount(prev => {
            console.log(`‚úÖ Succ√®s incr√©ment√©: ${prev} -> ${prev + 1}`);
            // Augmenter la difficult√© apr√®s plusieurs succ√®s
            if ((prev + 1) % 3 === 0) {
              setDifficulty(prevDiff => Math.min(prevDiff + 1, 2));
              console.log(`üîº Difficult√© augment√©e √† ${Math.min((prev + 1) % 3 + 1, 3)}`);
            }
            return prev + 1;
          });
          return 100; // R√©compense tr√®s √©lev√©e pour atteindre la cible
        }
        
        // R√©compense proportionnelle √† la proximit√© (plus simple)
        // Coefficient entre 0 et 1 bas√© sur la distance (1 = proche, 0 = loin)
        const proximityReward = Math.max(0, 1 - distance / 200);
        calculatedReward = proximityReward;
        
        // R√©compense additionnelle si l'agent se rapproche de la cible
        const distanceDelta = lastDistance.current - distance;
        if (distanceDelta > 0) {
          // Bonus pour se rapprocher
          calculatedReward += 0.2; 
        } else {
          // P√©nalit√© pour s'√©loigner
          calculatedReward -= 0.1;
        }
        
        // Mettre √† jour la derni√®re distance
        lastDistance.current = distance;
        
        // Log de d√©bogage
        if (episodeSteps % 60 === 0) {
          console.log(`Distance: ${distance.toFixed(2)}, R√©compense: ${calculatedReward.toFixed(2)}`);
        }
        
        // Mettre √† jour les stats pour l'UI
        setReward(calculatedReward);
        if (calculatedReward > bestReward) {
          setBestReward(calculatedReward);
        }
        
        return calculatedReward;
      },
      isDone: () => {
        // Un √©pisode se termine si:
        // 1. L'agent a atteint la cible
        // 2. Plus de 20 secondes se sont √©coul√©es
        // 3. L'agent est sorti du plateau (Ground)
        
        // V√©rification imm√©diate si l'agent est sorti du plateau
        const pos = bodyRef.current?.translation() || { x: 0, y: 0, z: 0 };
        
        // V√©rifier si l'agent est sorti du plateau
        if (pos.x < PLATEAU_LIMITS.minX || pos.x > PLATEAU_LIMITS.maxX ||
            pos.y < PLATEAU_LIMITS.minY || // V√©rifie s'il est tomb√© sous le ground
            pos.z < PLATEAU_LIMITS.minZ || pos.z > PLATEAU_LIMITS.maxZ) {
          
          console.log(`L'agent est sorti du plateau: [${pos.x.toFixed(2)}, ${pos.y.toFixed(2)}, ${pos.z.toFixed(2)}]`);
          return true; // Terminer l'√©pisode imm√©diatement
        }
        
        // V√©rifier si l'agent a atteint la cible
        const distance = Math.sqrt(
          Math.pow(pos.x - targetPosition[0], 2) +
          Math.pow(pos.y - targetPosition[1], 2) +
          Math.pow(pos.z - targetPosition[2], 2)
        );
        
        if (distance < 25) {
          console.log(`üéØ SUCC√àS! L'agent a atteint la cible!`);
          return true;
        }
        
        // V√©rifier le temps √©coul√© (limite √† 20 secondes)
        if (episodeTime >= 20) {
          console.log(`‚è±Ô∏è √âpisode termin√© apr√®s ${episodeTime.toFixed(1)} secondes`);
          return true;
        }
        
        return false;
      },
      onReset: () => {
        // R√©initialiser la position de l'agent selon la difficult√© actuelle
        const startPos = getStartingPosition();
        bodyRef.current?.setLinvel(new Vector3(0, 0, 0), true);
        bodyRef.current?.setTranslation(
          new Vector3(startPos[0], startPos[1], startPos[2]), 
          true
        );
        
        // Ne PAS mettre √† jour le nombre de succ√®s ici, car c'est d√©j√† fait dans computeReward
        
        // R√©initialiser les compteurs
        setEpisodeSteps(0);
        setReachedTarget(false);
        setEpisodeCount(prev => prev + 1);
        setEpisodeTime(0);
        setEpisodeStartTime(Date.now());
        lastDistance.current = Infinity;
        
        console.log(`√âpisode ${episodeCount} termin√©.`);
      },
      stepIntervalMs: 1000 / 60, // 60fps
    });
  }
  
  // Test de mouvement p√©riodique pour v√©rifier si l'agent peut bouger
  useFrame(({clock}) => {
    // V√©rifier toutes les 5 secondes si l'agent ne bouge pas
    if (isTraining && clock.getElapsedTime() % 5 < 0.1 && bodyRef.current) {
      const pos = bodyRef.current.translation();
      const vel = bodyRef.current.linvel();
      console.log(`‚è±Ô∏è Position actuelle: [${pos.x.toFixed(2)}, ${pos.y.toFixed(2)}, ${pos.z.toFixed(2)}]`);
      console.log(`üöÄ Vitesse actuelle: [${vel.x.toFixed(2)}, ${vel.y.toFixed(2)}, ${vel.z.toFixed(2)}]`);
      
      // Si la vitesse est presque nulle, tenter un mouvement de test
      if (Math.abs(vel.x) < 0.1 && Math.abs(vel.z) < 0.1) {
        console.log("üîÑ Test de mouvement (agent coinc√©?)");
        const testImpulse = MOVEMENT_SPEED * 10;
        bodyRef.current.applyImpulse(new Vector3(0, 0, -testImpulse), true);
      }
    }
  });
  
  // Boucle principale, mettre √† jour le temps de l'√©pisode
  useFrame(() => {
    if (isTraining && envRef.current) {
      // Mode entra√Ænement: IA contr√¥le l'agent
      envRef.current.step();
      
      // Mettre √† jour le temps √©coul√© de l'√©pisode
      const currentTime = Date.now();
      const elapsedSeconds = (currentTime - episodeStartTime) / 1000;
      setEpisodeTime(elapsedSeconds);
      
      // Forcer l'arr√™t de l'√©pisode apr√®s 20 secondes
      if (elapsedSeconds > 20 && envRef.current) {
        console.log("‚ö†Ô∏è Arr√™t forc√© de l'√©pisode apr√®s 20 secondes!");
        envRef.current.reset();
      }
    }
  });
  
  // Fonctions de contr√¥le
  const startTraining = () => {
    setIsTraining(true);
    if (envRef.current) {
      envRef.current.start();
    }
  };
  
  const stopTraining = () => {
    setIsTraining(false);
    if (envRef.current) {
      envRef.current.stop();
    }
  };
  
  const resetEnvironment = () => {
    if (envRef.current) {
      envRef.current.reset();
    }
  };
  
  return (
    <>
      <RigidBody 
        ref={bodyRef}
        position={getStartingPosition()}
        type="dynamic" 
        shape="cuboid"
        linearDamping={0.1}  // R√©duire l'amortissement pour que l'agent puisse bouger plus facilement
        mass={0.5}           // R√©duire la masse pour faciliter le mouvement 
        friction={0}
        lockRotations
      >
        <mesh castShadow>
          <boxGeometry args={[20, 20, 20]} />
          <meshStandardMaterial color="blue" />
        </mesh>
      </RigidBody>
      
      {/* Interface utilisateur pour contr√¥ler l'entra√Ænement */}
      <Html position={[100, 100, 0]} style={{ width: '300px', height: 'auto', background: 'rgba(0,0,0,0.7)', padding: '10px', borderRadius: '5px', color: 'white' }}>
        <div>
          <h3>Contr√¥le d'entra√Ænement</h3>
          <div>√âpisodes: {episodeCount}</div>
          <div>Succ√®s: {successCount} / {episodeCount}</div>
          <div>Difficult√©: {difficulty + 1}/3</div>
          <div>Temps: {episodeTime.toFixed(1)}s</div>
          <div>Derni√®re action: {lastAction !== -1 ? ['Gauche', 'Droite', 'Avant', 'Arri√®re'][lastAction] : 'Aucune'}</div>
          <div>R√©compense: {reward.toFixed(2)}</div>
          <div style={{ marginTop: '10px' }}>
            {!isTraining ? (
              <button onClick={startTraining}>D√©marrer l'entra√Ænement</button>
            ) : (
              <button onClick={stopTraining}>Arr√™ter l'entra√Ænement</button>
            )}
            <button onClick={resetEnvironment} style={{ marginLeft: '10px' }}>R√©initialiser</button>
          </div>
        </div>
      </Html>
    </>
  );
}

function Experience() {
  const cibleRef = useRef<RapierRigidBody | null>(null);
  const [targetPosition, setTargetPosition] = useState<[number, number, number]>([0, 10, 0]);
  
  useFrame(() => {
    if (cibleRef.current) {
      const pos = cibleRef.current.translation();
      setTargetPosition([pos.x, pos.y, pos.z]);
    }
  });

  return (
    <>
      <ambientLight intensity={0.3} />
      <directionalLight
        position={[10, 10, 10]}
        intensity={3}
        castShadow
        shadow-mapSize={[1024, 1024]}
      />
      <Agent targetPosition={targetPosition} />
      <Cible cibleRef={cibleRef} />
      <Ground />
    </>
  );
}

export default Experience;